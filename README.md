# CAption TRansformer
Image captioning using Transformer

## Methods
- This repo provides two ways of applying Transformer in Image Captioning with pretrained VIT Encoder or Bottom-Up Attention Encoder
- To extract bottom-up features, we provide [Colab Notebook](https://colab.research.google.com/drive/1xC4mVc_bp0t4-7T4xVum3AECFCalsgsv?usp=sharing) which adapts [Detectron model](https://github.com/airsplay/py-bottom-up-attention)

## Paper References
- Ideas from:
  - [CPTR: FULL TRANSFORMER NETWORK FOR IMAGE CAPTIONING](https://arxiv.org/pdf/2101.10804.pdf) (2021; Wei Liu, Sihan Chen et. al)
  - [Bottom-Up and Top-Down Attention for Image Captioning](https://arxiv.org/abs/1707.07998) (2018; Peter Anderson et. al)
  - Transformer Bottom-Up Attention: https://github.com/krasserm/fairseq-image-captioning

## Code References
- https://github.com/SamLynnEvans/Transformer
- https://nlp.seas.harvard.edu/2018/04/03/attention.html
- https://github.com/salaniz/pycocoevalcap
- https://huggingface.co/blog/how-to-generate
- https://github.com/krasserm/fairseq-image-captioning
- https://github.com/airsplay/py-bottom-up-attention
